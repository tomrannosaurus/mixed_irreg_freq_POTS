{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "yKwXH9DnR9j9",
        "outputId": "753fa351-3678-4f42-e0cf-abe07e86ebaf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pypots==0.8.1\n",
            "  Downloading pypots-0.8.1-py3-none-any.whl.metadata (42 kB)\n",
            "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/42.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.9/42.9 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from pypots==0.8.1) (3.12.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from pypots==0.8.1) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from pypots==0.8.1) (1.14.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from pypots==0.8.1) (1.13.1)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from pypots==0.8.1) (0.8.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from pypots==0.8.1) (2.2.2)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (from pypots==0.8.1) (0.13.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from pypots==0.8.1) (3.10.0)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.11/dist-packages (from pypots==0.8.1) (2.18.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from pypots==0.8.1) (1.6.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from pypots==0.8.1) (2.6.0+cu124)\n",
            "Collecting tsdb>=0.6.1 (from pypots==0.8.1)\n",
            "  Downloading tsdb-0.7.1-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting pygrinder>=0.6.4 (from pypots==0.8.1)\n",
            "  Downloading pygrinder-0.7-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting benchpots>=0.3 (from pypots==0.8.1)\n",
            "  Downloading benchpots-0.3.2-py3-none-any.whl.metadata (9.5 kB)\n",
            "Collecting ai4ts (from pypots==0.8.1)\n",
            "  Downloading ai4ts-0.0.3-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->pypots==0.8.1) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->pypots==0.8.1) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->pypots==0.8.1) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->pypots==0.8.1) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->pypots==0.8.1) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.10.0->pypots==0.8.1)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.10.0->pypots==0.8.1)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.10.0->pypots==0.8.1)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.10.0->pypots==0.8.1)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.10.0->pypots==0.8.1)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.10.0->pypots==0.8.1)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.10.0->pypots==0.8.1)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.10.0->pypots==0.8.1)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.10.0->pypots==0.8.1)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->pypots==0.8.1) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->pypots==0.8.1) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->pypots==0.8.1) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.10.0->pypots==0.8.1)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->pypots==0.8.1) (3.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->pypots==0.8.1) (1.3.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from tsdb>=0.6.1->pypots==0.8.1) (4.67.1)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.11/dist-packages (from tsdb>=0.6.1->pypots==0.8.1) (18.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from tsdb>=0.6.1->pypots==0.8.1) (2.32.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pypots==0.8.1) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pypots==0.8.1) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pypots==0.8.1) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pypots==0.8.1) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pypots==0.8.1) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pypots==0.8.1) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pypots==0.8.1) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pypots==0.8.1) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->pypots==0.8.1) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->pypots==0.8.1) (2025.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->pypots==0.8.1) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->pypots==0.8.1) (3.6.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard->pypots==0.8.1) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tensorboard->pypots==0.8.1) (1.71.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard->pypots==0.8.1) (3.7)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.11/dist-packages (from tensorboard->pypots==0.8.1) (4.25.6)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard->pypots==0.8.1) (75.1.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.11/dist-packages (from tensorboard->pypots==0.8.1) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard->pypots==0.8.1) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard->pypots==0.8.1) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard->pypots==0.8.1) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->tsdb>=0.6.1->pypots==0.8.1) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->tsdb>=0.6.1->pypots==0.8.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->tsdb>=0.6.1->pypots==0.8.1) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->tsdb>=0.6.1->pypots==0.8.1) (2025.1.31)\n",
            "Downloading pypots-0.8.1-py3-none-any.whl (497 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m497.6/497.6 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading benchpots-0.3.2-py3-none-any.whl (29 kB)\n",
            "Downloading pygrinder-0.7-py3-none-any.whl (24 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m52.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m73.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tsdb-0.7.1-py3-none-any.whl (32 kB)\n",
            "Downloading ai4ts-0.0.3-py3-none-any.whl (13 kB)\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, ai4ts, nvidia-cusparse-cu12, nvidia-cudnn-cu12, tsdb, nvidia-cusolver-cu12, pygrinder, benchpots, pypots\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed ai4ts-0.0.3 benchpots-0.3.2 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pygrinder-0.7 pypots-0.8.1 tsdb-0.7.1\n"
          ]
        }
      ],
      "source": [
        "# install PyPOTS\n",
        "!pip install pypots==0.8.1 # note: broken in current pypots=\n",
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# paths to PhysioNet 2012 data in Google Drive\n",
        "google_drive_folder = '/content/drive/MyDrive/BiTimelyGPT-main/'\n",
        "set_a_directory = f\"{google_drive_folder}/set-a\"\n",
        "set_b_directory = f\"{google_drive_folder}/set-b\"\n",
        "outcomes_a_file = f\"{set_a_directory}/Outcomes-a.txt\"\n",
        "outcomes_b_file = f\"{set_b_directory}/Outcomes-b.txt\"\n",
        "\n",
        "print(\"Set A Directory:\", set_a_directory)\n",
        "print(\"Set B Directory:\", set_b_directory)\n",
        "print(\"Outcomes A File:\", outcomes_a_file)\n",
        "print(\"Outcomes B File:\", outcomes_b_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Gva6Dem6SFJT",
        "outputId": "412cf741-a9ca-4889-d295-69e5a1bf8267"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Set A Directory: /content/drive/MyDrive/BiTimelyGPT-main//set-a\n",
            "Set B Directory: /content/drive/MyDrive/BiTimelyGPT-main//set-b\n",
            "Outcomes A File: /content/drive/MyDrive/BiTimelyGPT-main//set-a/Outcomes-a.txt\n",
            "Outcomes B File: /content/drive/MyDrive/BiTimelyGPT-main//set-b/Outcomes-b.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load outcomes\n",
        "outcomes_a = pd.read_csv(outcomes_a_file)\n",
        "outcomes_b = pd.read_csv(outcomes_b_file)\n",
        "\n",
        "# inspect data head\n",
        "print(\"Outcomes-a:\")\n",
        "print(outcomes_a.head())\n",
        "print(\"\\nOutcomes-b:\")\n",
        "print(outcomes_b.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "nTUvyN52SUf2",
        "outputId": "5cba58b8-80ad-4f72-8699-4f705d064716"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Outcomes-a:\n",
            "   RecordID  SAPS-I  SOFA  Length_of_stay  Survival  In-hospital_death\n",
            "0    132539       6     1               5        -1                  0\n",
            "1    132540      16     8               8        -1                  0\n",
            "2    132541      21    11              19        -1                  0\n",
            "3    132543       7     1               9       575                  0\n",
            "4    132545      17     2               4       918                  0\n",
            "\n",
            "Outcomes-b:\n",
            "   RecordID  SAPS-I  SOFA  Length_of_stay  Survival  In-hospital_death\n",
            "0    142675      27    14               9         7                  1\n",
            "1    142676      12     1              31       468                  0\n",
            "2    142680      12     7              17        16                  1\n",
            "3    142683      19    15              17        -1                  0\n",
            "4    142688       3     0               9        -1                  0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# add a 'set' column (so we can distinguish between set-a and set-b)\n",
        "outcomes_a['set'] = 'a'\n",
        "outcomes_b['set'] = 'b'\n",
        "\n",
        "# combine outcomes into a single df\n",
        "combined_outcomes = pd.concat([outcomes_a, outcomes_b], ignore_index=True)\n",
        "\n",
        "#verify object\n",
        "print(\"\\nCombined Outcomes:\")\n",
        "print(combined_outcomes.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "8gKH9W_qSXEO",
        "outputId": "698d4798-65cf-434e-dc4b-29f3823d74ba"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Combined Outcomes:\n",
            "   RecordID  SAPS-I  SOFA  Length_of_stay  Survival  In-hospital_death set\n",
            "0    132539       6     1               5        -1                  0   a\n",
            "1    132540      16     8               8        -1                  0   a\n",
            "2    132541      21    11              19        -1                  0   a\n",
            "3    132543       7     1               9       575                  0   a\n",
            "4    132545      17     2               4       918                  0   a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for binary classification we use 'In-hospital_death' as the target\n",
        "# remove records with 'In-hospital_death' == -1\n",
        "filtered_outcomes = combined_outcomes[combined_outcomes['In-hospital_death'] != -1].reset_index(drop=True)\n",
        "\n",
        "# verify filtering\n",
        "print(\"\\nFiltered Outcomes (no -1 in 'In-hospital_death'):\")\n",
        "print(filtered_outcomes.head())\n",
        "print(\"\\nNumber of records after filtering:\", len(filtered_outcomes))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "N04xMedHSY1u",
        "outputId": "4f7c3b3b-d0a8-4133-b4e8-e0010d7e4196"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Filtered Outcomes (no -1 in 'In-hospital_death'):\n",
            "   RecordID  SAPS-I  SOFA  Length_of_stay  Survival  In-hospital_death set\n",
            "0    132539       6     1               5        -1                  0   a\n",
            "1    132540      16     8               8        -1                  0   a\n",
            "2    132541      21    11              19        -1                  0   a\n",
            "3    132543       7     1               9       575                  0   a\n",
            "4    132545      17     2               4       918                  0   a\n",
            "\n",
            "Number of records after filtering: 8000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_patient_file(file_path):\n",
        "    \"\"\"\n",
        "    Parses an individual patient file and returns a DataFrame with time steps and features.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the patient file.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame containing time steps and features.\n",
        "    \"\"\"\n",
        "    # read data file\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # init dictionaries for static and dynamic features\n",
        "    static_features = {}\n",
        "    dynamic_features = {}\n",
        "\n",
        "    # iterate over rows\n",
        "    for index, row in df.iterrows():\n",
        "        time = row['Time']\n",
        "        param = row['Parameter']\n",
        "        value = row['Value']\n",
        "\n",
        "        # convert time to minutes\n",
        "        if ':' in time:\n",
        "            hours, minutes = map(int, time.split(':'))\n",
        "            total_minutes = hours * 60 + minutes\n",
        "        else:\n",
        "            try:\n",
        "                total_minutes = float(time)\n",
        "            except:\n",
        "                total_minutes = 0  # this sets default to 0 if conversion fails\n",
        "\n",
        "        # static features are at Time == 0\n",
        "        if total_minutes == 0:\n",
        "            static_features[param] = value\n",
        "        else:\n",
        "            if total_minutes not in dynamic_features:\n",
        "                dynamic_features[total_minutes] = {}\n",
        "            dynamic_features[total_minutes][param] = value\n",
        "\n",
        "    # convert dynamic_features to df\n",
        "    if dynamic_features:\n",
        "        dynamic_df = pd.DataFrame.from_dict(dynamic_features, orient='index').sort_index()\n",
        "        dynamic_df.index.name = 'Time'\n",
        "        dynamic_df.reset_index(inplace=True)\n",
        "    else:\n",
        "        dynamic_df = pd.DataFrame(columns=['Time'])\n",
        "\n",
        "    # add static features to df as columns\n",
        "    for key, val in static_features.items():\n",
        "        dynamic_df[key] = val\n",
        "\n",
        "    return dynamic_df"
      ],
      "metadata": {
        "id": "hxrJsAu2SbDH"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def load_all_patients(set_path, outcomes_df):\n",
        "    \"\"\"\n",
        "    Loads and parses all patient files in a given set (set-a or set-b).\n",
        "\n",
        "    Args:\n",
        "        set_path (str): Path to the set directory.\n",
        "        outcomes_df (pd.DataFrame): DataFrame containing outcomes for the set.\n",
        "\n",
        "    Returns:\n",
        "        list: List of patient DataFrames with features and time steps.\n",
        "        list: List of corresponding labels.\n",
        "    \"\"\"\n",
        "    patient_data = []\n",
        "    labels = []\n",
        "\n",
        "    # get list of per-patient files (excluding outcomes in Outcomes-a.txt and Outcomes-b.txt)\n",
        "    patient_files = [f for f in os.listdir(set_path) if f.endswith('.txt') and not f.startswith('Outcomes')]\n",
        "\n",
        "    for patient_file in tqdm(patient_files, desc=f\"Loading patients from {set_path}\"):\n",
        "        try:\n",
        "            record_id = int(patient_file.replace('.txt', ''))\n",
        "        except:\n",
        "            continue  # this skips files that don't have numeric RecordID\n",
        "\n",
        "        file_path = os.path.join(set_path, patient_file)\n",
        "\n",
        "        # parse patient file\n",
        "        patient_df = parse_patient_file(file_path)\n",
        "\n",
        "        # add RecordID for merge\n",
        "        patient_df['RecordID'] = record_id\n",
        "\n",
        "        # merge w/ outcomes to get the label\n",
        "        outcome = outcomes_df[outcomes_df['RecordID'] == record_id]\n",
        "        if outcome.empty:\n",
        "            continue  # this skips if no outcome found\n",
        "        label = outcome['In-hospital_death'].values[0]\n",
        "        patient_data.append(patient_df)\n",
        "        labels.append(label)\n",
        "\n",
        "    return patient_data, labels\n",
        "\n",
        "# load set-a (training)\n",
        "train_data, train_labels = load_all_patients(set_a_directory, outcomes_a)\n",
        "\n",
        "# load set-b (validation)\n",
        "val_data, val_labels = load_all_patients(set_b_directory, outcomes_b)\n",
        "\n",
        "# verify data opperations\n",
        "print(f\"\\nNumber of training samples: {len(train_data)}\")\n",
        "print(f\"Number of validation samples: {len(val_data)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "5tIBA5dFSfes",
        "outputId": "df206bf5-9111-4b8b-a3dc-51d30d199bc4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading patients from /content/drive/MyDrive/BiTimelyGPT-main//set-a: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4000/4000 [05:00<00:00, 13.30it/s]\n",
            "Loading patients from /content/drive/MyDrive/BiTimelyGPT-main//set-b: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4000/4000 [05:05<00:00, 13.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of training samples: 4000\n",
            "Number of validation samples: 4000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# identify static features (Physionet 2012)\n",
        "static_features = ['RecordID', 'Age', 'Gender', 'Height', 'ICUType', 'Weight']\n",
        "\n",
        "# dynamic features are all columns except 'Time' and static features\n",
        "dynamic_features = ['Albumin', 'ALP', 'ALT', 'AST', 'Bilirubin', 'BUN', 'Cholesterol',\n",
        "            'Creatinine', 'DiasABP', 'FiO2', 'GCS', 'Glucose', 'HCO3', 'HCT',\n",
        "            'HR', 'K', 'Lactate', 'Mg', 'MAP', 'MechVent', 'Na', 'NIDiasABP',\n",
        "            'NIMAP', 'NISysABP', 'PaCO2', 'PaO2', 'pH', 'Platelets', 'RespRate',\n",
        "            'SaO2', 'SysABP', 'Temp', 'TropI', 'TropT', 'Urine', 'WBC', 'Weight'\n",
        "        ]\n",
        "\n",
        "# verify static and dynamic variables\n",
        "print(\"Static Features:\", static_features)\n",
        "print(\"Dynamic Features:\", dynamic_features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "AYBm64_LSiRH",
        "outputId": "17b6cb7c-d91d-4ab7-9fe1-c3adbf9b6d2e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Static Features: ['RecordID', 'Age', 'Gender', 'Height', 'ICUType', 'Weight']\n",
            "Dynamic Features: ['Albumin', 'ALP', 'ALT', 'AST', 'Bilirubin', 'BUN', 'Cholesterol', 'Creatinine', 'DiasABP', 'FiO2', 'GCS', 'Glucose', 'HCO3', 'HCT', 'HR', 'K', 'Lactate', 'Mg', 'MAP', 'MechVent', 'Na', 'NIDiasABP', 'NIMAP', 'NISysABP', 'PaCO2', 'PaO2', 'pH', 'Platelets', 'RespRate', 'SaO2', 'SysABP', 'Temp', 'TropI', 'TropT', 'Urine', 'WBC', 'Weight']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "def create_sequences(patient_data, static_features, dynamic_features, labels, max_time_steps=None):\n",
        "    \"\"\"\n",
        "    Creates sequences for GRU-D from individual patient data.\n",
        "\n",
        "    Args:\n",
        "        patient_data (list): List of patient DataFrames.\n",
        "        static_features (list): List of static feature names.\n",
        "        dynamic_features (list): List of dynamic feature names.\n",
        "        labels (list): List of labels corresponding to each patient.\n",
        "        max_time_steps (int, optional): Maximum number of time steps. If None, use the maximum length.\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary containing 'X', 'mask', 'delta', and 'y'.\n",
        "    \"\"\"\n",
        "    X = []\n",
        "    mask = []\n",
        "    delta = []\n",
        "    y = []\n",
        "\n",
        "    num_features = len(dynamic_features)\n",
        "\n",
        "    for df, label in tqdm(zip(patient_data, labels), total=len(patient_data), desc=\"Creating sequences\"):\n",
        "        # sort df by time\n",
        "        df = df.sort_values('Time').reset_index(drop=True)\n",
        "\n",
        "        # check if static features are present\n",
        "        if df[static_features].isnull().all().all():\n",
        "            continue  # this skips patient if all static features are missing\n",
        "\n",
        "        # compute time differences (delta)\n",
        "        time = df['Time'].values\n",
        "        delta_time = np.diff(time, prepend=0)\n",
        "        delta_time = np.where(delta_time == 0, 1, delta_time)  # avoid zero delta, i.e. no duplicate measurments\n",
        "\n",
        "        # repeat delta_time for each feature to match (time_steps, num_features)\n",
        "        delta_repeated = np.tile(delta_time.reshape(-1, 1), (1, num_features))\n",
        "\n",
        "        # extract dynamic features, ensure all dynamic_features are present\n",
        "        dynamic = df.reindex(columns=dynamic_features).values.astype(float)  # note shape is: (time_steps, num_features)\n",
        "\n",
        "        # missing values: assuming missing if not present in the record\n",
        "        # create mask where 1 indicates observed and 0 indicates missing\n",
        "        dynamic_mask = ~np.isnan(dynamic)\n",
        "        dynamic_mask = dynamic_mask.astype(float)\n",
        "\n",
        "        # fill missing values with zero (note: PyPOTS GRU-D expects missing values to be filled)\n",
        "        dynamic_filled = np.nan_to_num(dynamic, nan=0.0)\n",
        "\n",
        "        # append to lists\n",
        "        X.append(dynamic_filled)\n",
        "        mask.append(dynamic_mask)\n",
        "        delta.append(delta_repeated)\n",
        "\n",
        "        # append label\n",
        "        y.append(label)\n",
        "\n",
        "    # determine maximum time steps (need this to limit memory issues for now)\n",
        "    if not max_time_steps:\n",
        "        max_time_steps = max([seq.shape[0] for seq in X])\n",
        "\n",
        "    # pad sequences\n",
        "    num_features = len(dynamic_features)\n",
        "\n",
        "    X_padded = np.zeros((len(X), max_time_steps, num_features))\n",
        "    mask_padded = np.zeros((len(mask), max_time_steps, num_features))\n",
        "    delta_padded = np.zeros((len(delta), max_time_steps, num_features))\n",
        "    y_array = np.array(y)\n",
        "\n",
        "    for i in range(len(X)):\n",
        "        seq_len = X[i].shape[0]\n",
        "        if seq_len > max_time_steps:\n",
        "            # truncate sequences longer than max_time_steps\n",
        "            X_padded[i, :max_time_steps, :] = X[i][:max_time_steps, :]\n",
        "            mask_padded[i, :max_time_steps, :] = mask[i][:max_time_steps, :]\n",
        "            delta_padded[i, :max_time_steps, :] = delta[i][:max_time_steps, :]\n",
        "        else:\n",
        "            # pad sequences shorter than max_time_steps\n",
        "            X_padded[i, :seq_len, :] = X[i]\n",
        "            mask_padded[i, :seq_len, :] = mask[i]\n",
        "            delta_padded[i, :seq_len, :] = delta[i]\n",
        "\n",
        "    return {\n",
        "        'X': X_padded,\n",
        "        'mask': mask_padded,\n",
        "        'delta': delta_padded,\n",
        "        'y': y_array\n",
        "    }\n",
        "\n",
        "# create sequences for training data\n",
        "train_sequences = create_sequences(train_data, static_features, dynamic_features, train_labels, max_time_steps=100)\n",
        "\n",
        "# create sequences for validation data\n",
        "val_sequences = create_sequences(val_data, static_features, dynamic_features, val_labels, max_time_steps=100)\n",
        "\n",
        "# ensure correct dims\n",
        "print(\"\\nTraining Data Shapes:\")\n",
        "for key in train_sequences:\n",
        "    print(f\"{key}: {train_sequences[key].shape}\")\n",
        "\n",
        "print(\"\\nValidation Data Shapes:\")\n",
        "for key in val_sequences:\n",
        "    print(f\"{key}: {val_sequences[key].shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "2LdRqZl8Sjxs",
        "outputId": "169023af-ee41-4cbe-f565-f2c95d5ba648"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Creating sequences: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4000/4000 [00:10<00:00, 370.99it/s]\n",
            "Creating sequences: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4000/4000 [00:09<00:00, 442.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training Data Shapes:\n",
            "X: (3997, 100, 37)\n",
            "mask: (3997, 100, 37)\n",
            "delta: (3997, 100, 37)\n",
            "y: (3997,)\n",
            "\n",
            "Validation Data Shapes:\n",
            "X: (3993, 100, 37)\n",
            "mask: (3993, 100, 37)\n",
            "delta: (3993, 100, 37)\n",
            "y: (3993,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# verify that labels are binary\n",
        "unique_train_labels = np.unique(train_sequences['y'])\n",
        "unique_val_labels = np.unique(val_sequences['y'])\n",
        "\n",
        "print(\"Unique labels in training set:\", unique_train_labels)\n",
        "print(\"Unique labels in validation set:\", unique_val_labels)\n",
        "\n",
        "# function to force binary labeling if extrenous values found\n",
        "if set(unique_train_labels) == {-1, 1}:\n",
        "    train_sequences['y'] = (train_sequences['y'] == 1).astype(int)\n",
        "    val_sequences['y'] = (val_sequences['y'] == 1).astype(int)\n",
        "    print(\"Mapped labels from {-1, 1} to {0, 1}\")\n",
        "elif set(unique_train_labels) == {0, 1}:\n",
        "    print(\"Labels are already in binary format (0 and 1)\")\n",
        "else:\n",
        "    raise ValueError(\"Unexpected label values. Please ensure labels are binary (0 and 1).\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "UwTXi7NiSmVh",
        "outputId": "3c0c5087-ccf5-4c1c-b4f1-05fa0101ac6c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique labels in training set: [0 1]\n",
            "Unique labels in validation set: [0 1]\n",
            "Labels are already in binary format (0 and 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare datasets in dictionary format (expected input for PyPOTS GRU-D)\n",
        "dataset_for_training = {\n",
        "    \"X\": train_sequences['X'],\n",
        "    \"mask\": train_sequences['mask'],\n",
        "    \"delta\": train_sequences['delta'],\n",
        "    \"y\": train_sequences['y']\n",
        "}\n",
        "\n",
        "dataset_for_validating = {\n",
        "    \"X\": val_sequences['X'],\n",
        "    \"mask\": val_sequences['mask'],\n",
        "    \"delta\": val_sequences['delta'],\n",
        "    \"y\": val_sequences['y']\n",
        "}\n",
        "\n",
        "print(\"Training dataset keys:\", dataset_for_training.keys())\n",
        "print(\"Validation dataset keys:\", dataset_for_validating.keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "SPgv2erfSowd",
        "outputId": "05805a27-5448-4371-fa70-1be3909df61d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training dataset keys: dict_keys(['X', 'mask', 'delta', 'y'])\n",
            "Validation dataset keys: dict_keys(['X', 'mask', 'delta', 'y'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pypots.optim import Adam\n",
        "from pypots.classification import GRUD\n",
        "\n",
        "# define model parameters\n",
        "n_steps = train_sequences['X'].shape[1] # max number of time steps in input data\n",
        "n_features = train_sequences['X'].shape[2] # num of dynamic features\n",
        "n_classes = 2 # 2 for binary classification\n",
        "\n",
        "# init the GRU-D model\n",
        "grud = GRUD(\n",
        "    n_steps=n_steps,\n",
        "    n_features=n_features,\n",
        "    n_classes=n_classes,\n",
        "    rnn_hidden_size=32,\n",
        "    batch_size=32,\n",
        "    epochs=100,\n",
        "    patience=10,\n",
        "    optimizer=Adam(lr=1e-3),\n",
        "    num_workers=0,\n",
        "    device=None,\n",
        "    saving_path=\"drive/MyDrive/classification/grud\",\n",
        "    model_saving_strategy=\"best\",\n",
        ")\n",
        "\n",
        "print(grud)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "_eugH5vCxLme",
        "outputId": "246fd134-4e8f-45d8-f611-4b234c529458"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-03-19 05:45:55 [WARNING]: â€¼ï¸ PyPOTS Ecosystem configuration file does not exist.\n",
            "2025-03-19 05:45:55 [INFO]: Wrote new configs to config.ini successfully.\n",
            "2025-03-19 05:45:55 [INFO]: ğŸ’« Initialized PyPOTS Ecosystem configuration file /root/.pypots/config.ini successfully.\n",
            "/usr/local/lib/python3.11/dist-packages/pypots/nn/modules/reformer/local_attention.py:31: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  @autocast(enabled=False)\n",
            "/usr/local/lib/python3.11/dist-packages/pypots/nn/modules/reformer/local_attention.py:98: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  @autocast(enabled=False)\n",
            "2025-03-19 05:46:09 [INFO]: No given device, using default device: cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\n",
            "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—\n",
            "â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘\n",
            "   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘\n",
            "   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘\n",
            "   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘\n",
            "   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•\n",
            "ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai \u001b[0m\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-03-19 05:46:09 [INFO]: Model files will be saved to drive/MyDrive/classification/grud/20250319_T054609\n",
            "2025-03-19 05:46:09 [INFO]: Tensorboard file will be saved to drive/MyDrive/classification/grud/20250319_T054609/tensorboard\n",
            "2025-03-19 05:46:10 [INFO]: GRUD initialized with the given hyperparameters, the number of trainable parameters: 16,128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<pypots.classification.grud.model.GRUD object at 0x7ccb26f10690>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train on training set, validate on validation set\n",
        "grud.fit(train_set=dataset_for_training, val_set=dataset_for_validating)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "U0I3Dgq4SqtM",
        "outputId": "4032e624-bf5c-4c80-9ee4-1022c48ed156"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-03-19 05:47:10 [INFO]: Epoch 001 - training loss: 0.4104, validation loss: 0.4036\n",
            "2025-03-19 05:47:35 [INFO]: Epoch 002 - training loss: 0.3927, validation loss: 0.3920\n",
            "2025-03-19 05:48:02 [INFO]: Epoch 003 - training loss: 0.3817, validation loss: 0.3882\n",
            "2025-03-19 05:48:26 [INFO]: Epoch 004 - training loss: 0.3750, validation loss: 0.3842\n",
            "2025-03-19 05:48:51 [INFO]: Epoch 005 - training loss: 0.3710, validation loss: 0.3798\n",
            "2025-03-19 05:49:16 [INFO]: Epoch 006 - training loss: 0.3645, validation loss: 0.3798\n",
            "2025-03-19 05:49:41 [INFO]: Epoch 007 - training loss: 0.3606, validation loss: 0.3739\n",
            "2025-03-19 05:50:05 [INFO]: Epoch 008 - training loss: 0.3566, validation loss: 0.3801\n",
            "2025-03-19 05:50:30 [INFO]: Epoch 009 - training loss: 0.3496, validation loss: 0.3787\n",
            "2025-03-19 05:50:58 [INFO]: Epoch 010 - training loss: 0.3475, validation loss: 0.3683\n",
            "2025-03-19 05:51:23 [INFO]: Epoch 011 - training loss: 0.3401, validation loss: 0.3777\n",
            "2025-03-19 05:51:48 [INFO]: Epoch 012 - training loss: 0.3389, validation loss: 0.3694\n",
            "2025-03-19 05:52:15 [INFO]: Epoch 013 - training loss: 0.3318, validation loss: 0.3698\n",
            "2025-03-19 05:52:40 [INFO]: Epoch 014 - training loss: 0.3265, validation loss: 0.3756\n",
            "2025-03-19 05:53:04 [INFO]: Epoch 015 - training loss: 0.3202, validation loss: 0.3706\n",
            "2025-03-19 05:53:29 [INFO]: Epoch 016 - training loss: 0.3163, validation loss: 0.3735\n",
            "2025-03-19 05:53:54 [INFO]: Epoch 017 - training loss: 0.3135, validation loss: 0.3787\n",
            "2025-03-19 05:54:19 [INFO]: Epoch 018 - training loss: 0.3117, validation loss: 0.3822\n",
            "2025-03-19 05:54:44 [INFO]: Epoch 019 - training loss: 0.3085, validation loss: 0.3756\n",
            "2025-03-19 05:55:09 [INFO]: Epoch 020 - training loss: 0.3014, validation loss: 0.3760\n",
            "2025-03-19 05:55:09 [INFO]: Exceeded the training patience. Terminating the training procedure...\n",
            "2025-03-19 05:55:09 [INFO]: Finished training. The best model is from epoch#10.\n",
            "2025-03-19 05:55:09 [INFO]: Saved the model to drive/MyDrive/classification/grud/20250319_T054609/GRUD.pypots\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load the best model\n",
        "grud.load('drive/MyDrive/classification/grud/20250319_T054609/GRUD.pypots')\n",
        "\n",
        "# Prepare test dataset in dictionary format\n",
        "dataset_for_testing = {\n",
        "    \"X\": val_sequences['X'],\n",
        "    \"mask\": val_sequences['mask'],\n",
        "    \"delta\": val_sequences['delta'],\n",
        "    \"y\": val_sequences['y']\n",
        "}\n",
        "\n",
        "\n",
        "# Predict on the testing set\n",
        "grud_results = grud.predict(dataset_for_testing)\n",
        "grud_prediction = grud_results[\"classification\"]\n",
        "\n",
        "from pypots.utils.metrics import calc_binary_classification_metrics\n",
        "\n",
        "# Calculate binary classification metrics\n",
        "metrics = calc_binary_classification_metrics(grud_prediction, dataset_for_testing[\"y\"])\n",
        "\n",
        "print(\"Testing classification metrics: \\n\"\n",
        "      f'ROC_AUC: {metrics[\"roc_auc\"]}, \\n'\n",
        "      f'PR_AUC: {metrics[\"pr_auc\"]},\\n'\n",
        "      f'F1: {metrics[\"f1\"]},\\n'\n",
        "      f'Precision: {metrics[\"precision\"]},\\n'\n",
        "      f'Recall: {metrics[\"recall\"]},\\n')"
      ],
      "metadata": {
        "id": "VBWoyyAUSsph",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "7083a098-c6d3-459c-d5de-03995d59f406"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing classification metrics: \n",
            "ROC_AUC: 0.7344001233679449, \n",
            "PR_AUC: 0.3148298294203433,\n",
            "F1: 0.19241192411924118,\n",
            "Precision: 0.4176470588235294,\n",
            "Recall: 0.125,\n",
            "\n"
          ]
        }
      ]
    }
  ]
}